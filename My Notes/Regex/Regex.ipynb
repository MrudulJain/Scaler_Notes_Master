{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8430b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mrudul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mrudul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mrudul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mrudul\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer # Word net is a corpus of root words in its simplest form \n",
    "\n",
    "nltk.download('stopwords') # Installs the Corpus (collection) of common words   \n",
    "nltk.download('punkt') #tokenizer method which uses unsupervised algorithm to create word into tokens\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b780353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None , ['Christmas Quote ']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "text = ''' \n",
    "\"One cannot have enough socks with him\", said Dora. \n",
    "\"Another has come and gone and I didn't get a single pair. \n",
    "People will keep on insisting on giving me books.\" \n",
    "Christmas Quote \n",
    "''' \n",
    "regex = 'Christ.*' \n",
    "print(re.match(regex, text),\",\",re.findall(regex, text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a84def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\"One cannot have enough socks with him\", said Dora. \\n\"Another has come and gone and I didn\\'t get a single pair. \\nPeople will keep on insisting on giving me books.\" \\nChristmas Quote \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b8288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aab']\n"
     ]
    }
   ],
   "source": [
    "regex = 'a{1,2}b'\n",
    "text = 'aaab'\n",
    "print(re.findall(regex, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92411692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 7), match='aaaaab'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c3684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'book', 'book']\n"
     ]
    }
   ],
   "source": [
    "text = '''The intellectual content in a physical books need not be a composition, nor even be called a book. Books can consist only of drawings, engravings or photographs, crossword puzzles or cut-out dolls. In a physical book, the pages can be left blank or can feature an abstract set of lines to support entries. '''\n",
    "print(re.findall('b..k', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97ed3feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 29), match='cryptographic encryption coin'>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"cryptographic encryption coin methods that can be cracked easily with quantum computers\" \n",
    "pattern = re.compile(\"crypto(.{1,30})coin\")\n",
    "print(pattern.match(sentence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bab7e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possibility dream come true make life interesting'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"It is the possibility of having a dream come true that makes life interesting.\"\n",
    "\n",
    "def preprocess(input_text):\n",
    "\n",
    "  # to lower cases and tokenization\n",
    "  text= input_text.lower()\n",
    "  text= word_tokenize(text)\n",
    "  \n",
    "  #stopwords removal\n",
    "  stop_w= stopwords.words(\"english\")\n",
    "  stop_w.extend([\".\"])\n",
    "  text_stopwords=[word for word in text if word not in stop_w]\n",
    "\n",
    "  text= text_stopwords\n",
    "\n",
    "  #Lemmatization\n",
    "  lemma = WordNetLemmatizer()\n",
    "  lem_words= [lemma.lemmatize(word) for word in text]\n",
    "\n",
    "  lemmatization_output= \" \".join(lem_words)\n",
    "\n",
    "  return(lemmatization_output)\n",
    "\n",
    "preprocess(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb2e53f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'cheetah is faster than a leopard'\n",
    "matched = re.search('is', sentence)\n",
    "print(matched.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ans = \"Ram is an obedient and smart student\"\n",
    "A= \"Ram is a good student and he is scoring really well in his class\"\n",
    "B= \"Ram is very good at studies and he is also a great sportsman\"\n",
    "C= \"Best football player in school is Ram\"\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "docs = [\n",
    "        ' '.join(ans),\n",
    "        ' '.join(A),\n",
    "        ' '.join(B),\n",
    "        ' '.join(C)\n",
    "]\n",
    "\n",
    "x1 = vectorizer.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73864a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False False True True "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "new_input = ['18:29', '23:55', '123', 'ab:de', '18:299', '99:99']\n",
    "results = lambda x: re.match('[0-9]{2}:[0-9]{2}', x) != None\n",
    "for x in new_input:\n",
    "  print(results(x),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a5d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
